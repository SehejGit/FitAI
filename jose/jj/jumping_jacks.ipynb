{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99da1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f934a",
   "metadata": {},
   "source": [
    "# Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655a85fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746083306.169388 1974631 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfa067",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b8af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "def calculate_distance(p1, p2):\n",
    "    \"\"\"Helper function to calculate distance between two points\"\"\"\n",
    "    return math.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)\n",
    "\n",
    "def normalized_to_pixel_coordinates(normalized_x, normalized_y, image_width, image_height):\n",
    "    \"\"\"Convert normalized coordinates to pixel coordinates\"\"\"\n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    return x_px, y_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3901e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_jumping_jacks(video_path, output_video_path=None, debug=False, already_positioned=False):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Setup output video writer if path is provided\n",
    "    if output_video_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Variables to track jumping jack state\n",
    "    jumping_jack_count = 0\n",
    "    jumping_jack_stage = None  # \"spread\" or \"together\"\n",
    "    frames_without_detection = 0\n",
    "    good_frames = 0\n",
    "    \n",
    "    # Flag to determine if the first cycle has been completed\n",
    "    first_cycle_complete = already_positioned  # Skip initialization if already positioned\n",
    "    \n",
    "    # Lists to store measurements for analysis\n",
    "    arm_spreads = []  # Distance between wrists relative to shoulder width\n",
    "    leg_spreads = []  # Distance between ankles relative to hip width\n",
    "    \n",
    "    # For tracking state transitions\n",
    "    state_changes = []\n",
    "    \n",
    "    # For smoothing measurements\n",
    "    recent_arm_spreads = []\n",
    "    recent_leg_spreads = []\n",
    "    window_size = 5  # Number of frames to average\n",
    "    \n",
    "    # debugging info - might delete this later\n",
    "    print(f\"Video dimensions: {frame_width}x{frame_height}, FPS: {fps}\")\n",
    "    print(f\"Already positioned mode: {already_positioned}\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "            \n",
    "        # Convert to RGB for MediaPipe\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "        \n",
    "        # Draw pose landmarks on the image\n",
    "        annotated_image = image.copy()\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            good_frames += 1\n",
    "            frames_without_detection = 0\n",
    "            \n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_image, \n",
    "                results.pose_landmarks, \n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "            \n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Debug output to see what landmarks are detected\n",
    "            if good_frames == 1:\n",
    "                print(f\"Detected {len(landmarks)} landmarks\")\n",
    "            \n",
    "            # Get key points for jumping jack analysis\n",
    "            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n",
    "                         landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n",
    "            right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n",
    "            left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                       landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "            left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                         landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "            right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "            \n",
    "            # Calculate normalized distances\n",
    "            shoulder_width = calculate_distance(left_shoulder, right_shoulder)\n",
    "            hip_width = calculate_distance(left_hip, right_hip)\n",
    "            \n",
    "            wrist_distance = calculate_distance(left_wrist, right_wrist)\n",
    "            ankle_distance = calculate_distance(left_ankle, right_ankle)\n",
    "            \n",
    "            # Normalize by shoulder/hip width to account for different distances from camera\n",
    "            arm_spread_ratio = wrist_distance / max(shoulder_width, 0.01)  # Avoid division by zero\n",
    "            leg_spread_ratio = ankle_distance / max(hip_width, 0.01)\n",
    "            \n",
    "            # Apply smoothing with a rolling window\n",
    "            recent_arm_spreads.append(arm_spread_ratio)\n",
    "            recent_leg_spreads.append(leg_spread_ratio)\n",
    "            \n",
    "            if len(recent_arm_spreads) > window_size:\n",
    "                recent_arm_spreads.pop(0)\n",
    "                recent_leg_spreads.pop(0)\n",
    "                \n",
    "            # Get smoothed values\n",
    "            smoothed_arm_ratio = sum(recent_arm_spreads) / len(recent_arm_spreads)\n",
    "            smoothed_leg_ratio = sum(recent_leg_spreads) / len(recent_leg_spreads)\n",
    "            \n",
    "            # Use smoothed values for state detection but store original for analysis\n",
    "            arm_spreads.append(arm_spread_ratio)\n",
    "            leg_spreads.append(leg_spread_ratio)\n",
    "\n",
    "            # Visualize arm and leg spread\n",
    "            cv2.putText(annotated_image, \n",
    "                        f\"Arm spread: {arm_spread_ratio:.2f}x\",\n",
    "                        (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            cv2.putText(annotated_image, \n",
    "                        f\"Leg spread: {leg_spread_ratio:.2f}x\",\n",
    "                        (10, 60), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Determine jumping jack stage using smoothed values\n",
    "            # For arms: calculate the vertical position of wrists relative to shoulders\n",
    "            # We want to detect if arms are raised up\n",
    "            left_wrist_above = left_wrist[1] < left_shoulder[1] - 0.05  # Ensure wrist is clearly above shoulder\n",
    "            right_wrist_above = right_wrist[1] < right_shoulder[1] - 0.05\n",
    "            \n",
    "            # Calculate vertical position of wrists\n",
    "            wrist_height_ratio = ((left_shoulder[1] - left_wrist[1]) + (right_shoulder[1] - right_wrist[1])) / 2\n",
    "            \n",
    "            # Combined condition for the \"spread\" position - Check EITHER arms are up OR legs are wide\n",
    "            is_spread_position = False\n",
    "            \n",
    "            # Main condition: if legs are significantly spread OR arms are raised high\n",
    "            if smoothed_leg_ratio > 1.8 or (left_wrist_above and right_wrist_above and smoothed_arm_ratio > 1.5):\n",
    "                is_spread_position = True\n",
    "                \n",
    "            # Combined condition for the \"together\" position\n",
    "            # Legs close together AND arms not raised\n",
    "            is_together_position = smoothed_leg_ratio < 1.4 and not (left_wrist_above and right_wrist_above)\n",
    "            \n",
    "            # Add initialization status to the state display\n",
    "            state_text = f\"Spread: {is_spread_position}, Together: {is_together_position}, Stage: {jumping_jack_stage}\"\n",
    "            if not first_cycle_complete:\n",
    "                state_text += \" (Initializing)\"\n",
    "                \n",
    "            cv2.putText(annotated_image, state_text, (10, frame_height - 20), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Additional debugging info\n",
    "            metrics_text = f\"Arm: {smoothed_arm_ratio:.2f}x, Leg: {smoothed_leg_ratio:.2f}x\"\n",
    "            cv2.putText(annotated_image, metrics_text, (10, frame_height - 40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # State machine for counting - Modified to handle initialization\n",
    "            if is_spread_position and (jumping_jack_stage == \"together\" or jumping_jack_stage is None):\n",
    "                old_stage = jumping_jack_stage\n",
    "                jumping_jack_stage = \"spread\"\n",
    "                cv2.putText(annotated_image, 'SPREAD', (50, 90), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                # Log state transition\n",
    "                transition_msg = f\"Frame {good_frames}: {old_stage} -> spread, arm: {smoothed_arm_ratio:.2f}, leg: {smoothed_leg_ratio:.2f}\"\n",
    "                state_changes.append(transition_msg)\n",
    "                if debug:\n",
    "                    print(transition_msg)\n",
    "            \n",
    "            elif is_together_position and jumping_jack_stage == \"spread\":\n",
    "                old_stage = jumping_jack_stage\n",
    "                jumping_jack_stage = \"together\"\n",
    "                \n",
    "                # Only increment count if we've completed initialization\n",
    "                if first_cycle_complete:\n",
    "                    jumping_jack_count += 1\n",
    "                    transition_msg = f\"Frame {good_frames}: spread -> together, COUNTED JUMPING JACK #{jumping_jack_count}, arm: {smoothed_arm_ratio:.2f}, leg: {smoothed_leg_ratio:.2f}\"\n",
    "                else:\n",
    "                    first_cycle_complete = True  # Mark that initialization is complete\n",
    "                    transition_msg = f\"Frame {good_frames}: spread -> together, INITIALIZATION COMPLETE (not counted), arm: {smoothed_arm_ratio:.2f}, leg: {smoothed_leg_ratio:.2f}\"\n",
    "                \n",
    "                state_changes.append(transition_msg)\n",
    "                if debug:\n",
    "                    print(transition_msg)\n",
    "                    \n",
    "                cv2.putText(annotated_image, 'TOGETHER', (50, 90), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            frames_without_detection += 1\n",
    "            if frames_without_detection % 30 == 0:\n",
    "                print(f\"No pose detection for {frames_without_detection} frames\")\n",
    "                \n",
    "        # Display jumping jack count and initialization status\n",
    "        count_text = f'Jumping Jacks: {jumping_jack_count}'\n",
    "        if not first_cycle_complete:\n",
    "            count_text += \" (Initializing...)\"\n",
    "            \n",
    "        cv2.putText(annotated_image, count_text, (10, 120), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                    \n",
    "        # Write frame to output video\n",
    "        if output_video_path:\n",
    "            out.write(annotated_image)\n",
    "                \n",
    "    cap.release()\n",
    "    if output_video_path:\n",
    "        out.release()\n",
    "    \n",
    "    # Only analyze if we have enough valid frames\n",
    "    if good_frames < 10:\n",
    "        return {\n",
    "            \"jumping_jack_count\": 0,\n",
    "            \"error\": \"Not enough valid pose detections. Check video quality and positioning.\"\n",
    "        }\n",
    "        \n",
    "    # Analyze the data collected\n",
    "    max_arm_spread = max(arm_spreads) if arm_spreads else 0\n",
    "    min_arm_spread = min(arm_spreads) if arm_spreads else 0\n",
    "    max_leg_spread = max(leg_spreads) if leg_spreads else 0\n",
    "    min_leg_spread = min(leg_spreads) if leg_spreads else 0\n",
    "    \n",
    "    # Generate feedback\n",
    "    feedback = {\n",
    "        \"jumping_jack_count\": jumping_jack_count,\n",
    "        \"form_analysis\": {\n",
    "            \"max_arm_spread_ratio\": max_arm_spread,\n",
    "            \"min_arm_spread_ratio\": min_arm_spread,\n",
    "            \"max_leg_spread_ratio\": max_leg_spread,\n",
    "            \"min_leg_spread_ratio\": min_leg_spread,\n",
    "            \"frames_analyzed\": good_frames\n",
    "        },\n",
    "        \"feedback\": [],\n",
    "        \"state_transitions\": state_changes[:20]  # Include up to 20 state transitions for debugging\n",
    "    }\n",
    "    \n",
    "    if max_arm_spread < 1.8:\n",
    "        feedback[\"feedback\"].append(\"Try to raise your arms higher. For optimal form, your hands should meet or nearly meet above your head.\")\n",
    "    \n",
    "    if max_leg_spread < 1.8:\n",
    "        feedback[\"feedback\"].append(\"Your legs could spread wider for a more effective jumping jack.\")\n",
    "    \n",
    "    if min_arm_spread > 1.0:\n",
    "        feedback[\"feedback\"].append(\"Make sure to bring your arms all the way down to your sides in the starting position.\")\n",
    "    \n",
    "    if min_leg_spread > 1.1:\n",
    "        feedback[\"feedback\"].append(\"Try to bring your feet closer together in the starting position.\")\n",
    "    \n",
    "    if not feedback[\"feedback\"]:\n",
    "        feedback[\"feedback\"].append(\"Great form! Your jumping jacks have good range of motion in both arms and legs.\")\n",
    "        \n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ee69b",
   "metadata": {},
   "source": [
    "## if user needs to position themselves first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569d1cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746083306.375374 1974722 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video dimensions: 1280x720, FPS: 29\n",
      "Already positioned mode: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746083306.401409 1974718 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746083306.849472 1974721 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 33 landmarks\n",
      "Frame 1: None -> spread, arm: 1.45, leg: 1.87\n",
      "Frame 7: spread -> together, INITIALIZATION COMPLETE (not counted), arm: 1.55, leg: 1.33\n",
      "Frame 42: together -> spread, arm: 0.81, leg: 1.96\n",
      "Frame 60: spread -> together, COUNTED JUMPING JACK #1, arm: 2.33, leg: 1.08\n",
      "Frame 70: together -> spread, arm: 3.05, leg: 2.17\n",
      "Frame 88: spread -> together, COUNTED JUMPING JACK #2, arm: 2.69, leg: 1.27\n",
      "Frame 98: together -> spread, arm: 2.99, leg: 1.82\n",
      "Frame 117: spread -> together, COUNTED JUMPING JACK #3, arm: 2.17, leg: 1.02\n",
      "Frame 127: together -> spread, arm: 2.95, leg: 2.13\n",
      "Frame 145: spread -> together, COUNTED JUMPING JACK #4, arm: 2.60, leg: 1.37\n",
      "Frame 155: together -> spread, arm: 3.37, leg: 2.28\n",
      "Frame 173: spread -> together, COUNTED JUMPING JACK #5, arm: 2.73, leg: 1.23\n",
      "Frame 183: together -> spread, arm: 3.11, leg: 1.87\n",
      "Frame 202: spread -> together, COUNTED JUMPING JACK #6, arm: 2.36, leg: 1.11\n",
      "Frame 212: together -> spread, arm: 2.95, leg: 1.92\n",
      "Frame 231: spread -> together, COUNTED JUMPING JACK #7, arm: 2.46, leg: 1.05\n",
      "Frame 241: together -> spread, arm: 2.84, leg: 2.40\n",
      "Frame 259: spread -> together, COUNTED JUMPING JACK #8, arm: 3.08, leg: 1.15\n",
      "\n",
      "=== ANALYSIS RESULTS (Positioning First) ===\n",
      "Counted 8 jumping jacks\n",
      "Maximum arm spread: 3.55x shoulder width\n",
      "Maximum leg spread: 4.67x hip width\n",
      "Frames analyzed: 321\n",
      "\n",
      "Feedback:\n",
      "- Great form! Your jumping jacks have good range of motion in both arms and legs.\n"
     ]
    }
   ],
   "source": [
    "result1 = analyze_jumping_jacks(\"jj/jumping_jacks.mp4\", \"analyzed_positioning_first.mp4\", debug=True, already_positioned=False)\n",
    "\n",
    "print(\"\\n=== ANALYSIS RESULTS (Positioning First) ===\")\n",
    "print(f\"Counted {result1.get('jumping_jack_count', 0)} jumping jacks\")\n",
    "if \"error\" in result1:\n",
    "    print(f\"Error: {result1['error']}\")\n",
    "elif \"form_analysis\" in result1:\n",
    "    print(f\"Maximum arm spread: {result1['form_analysis']['max_arm_spread_ratio']:.2f}x shoulder width\")\n",
    "    print(f\"Maximum leg spread: {result1['form_analysis']['max_leg_spread_ratio']:.2f}x hip width\")\n",
    "    print(f\"Frames analyzed: {result1['form_analysis']['frames_analyzed']}\")\n",
    "    print(\"\\nFeedback:\")\n",
    "    for item in result1[\"feedback\"]:\n",
    "        print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4ee9d",
   "metadata": {},
   "source": [
    "## if user is already in position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5f0c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video dimensions: 1280x720, FPS: 30\n",
      "Already positioned mode: True\n",
      "Detected 33 landmarks\n",
      "Frame 28: None -> spread, arm: 3.46, leg: 2.05\n",
      "Frame 44: spread -> together, COUNTED JUMPING JACK #1, arm: 2.81, leg: 1.04\n",
      "Frame 54: together -> spread, arm: 3.43, leg: 1.83\n",
      "Frame 70: spread -> together, COUNTED JUMPING JACK #2, arm: 3.32, leg: 1.23\n",
      "Frame 81: together -> spread, arm: 3.57, leg: 1.83\n",
      "Frame 97: spread -> together, COUNTED JUMPING JACK #3, arm: 3.41, leg: 1.31\n",
      "Frame 109: together -> spread, arm: 3.47, leg: 1.99\n",
      "Frame 126: spread -> together, COUNTED JUMPING JACK #4, arm: 3.15, leg: 1.03\n",
      "Frame 137: together -> spread, arm: 3.48, leg: 1.94\n",
      "Frame 155: spread -> together, COUNTED JUMPING JACK #5, arm: 3.52, leg: 1.16\n",
      "\n",
      "=== ANALYSIS RESULTS (Already Positioned) ===\n",
      "Counted 5 jumping jacks\n",
      "Maximum arm spread: 3.77x shoulder width\n",
      "Maximum leg spread: 4.31x hip width\n",
      "Frames analyzed: 203\n",
      "\n",
      "Feedback:\n",
      "- Great form! Your jumping jacks have good range of motion in both arms and legs.\n"
     ]
    }
   ],
   "source": [
    "# if user already in position\n",
    "result2 = analyze_jumping_jacks(\"jj/dom_jj.mp4\", \"analyzed_already_positioned.mp4\", debug=True, already_positioned=True)\n",
    "\n",
    "print(\"\\n=== ANALYSIS RESULTS (Already Positioned) ===\")\n",
    "print(f\"Counted {result2.get('jumping_jack_count', 0)} jumping jacks\")\n",
    "if \"error\" in result2:\n",
    "    print(f\"Error: {result2['error']}\")\n",
    "elif \"form_analysis\" in result2:\n",
    "    print(f\"Maximum arm spread: {result2['form_analysis']['max_arm_spread_ratio']:.2f}x shoulder width\")\n",
    "    print(f\"Maximum leg spread: {result2['form_analysis']['max_leg_spread_ratio']:.2f}x hip width\")\n",
    "    print(f\"Frames analyzed: {result2['form_analysis']['frames_analyzed']}\")\n",
    "    print(\"\\nFeedback:\")\n",
    "    for item in result2[\"feedback\"]:\n",
    "        print(f\"- {item}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
