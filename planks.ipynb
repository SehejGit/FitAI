{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab344558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735d330",
   "metadata": {},
   "source": [
    "# Planks - Video Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd940649",
   "metadata": {},
   "source": [
    "- planks are good, just fix up code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a39c96e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745963794.034249 1418167 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
    "\n",
    "def analyze_plank(video_path, output_video_path=None):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    if output_video_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    plank_active = False\n",
    "    start_time = None\n",
    "    duration = 0\n",
    "    frames_without_detection = 0\n",
    "    good_frames = 0\n",
    "    alignment_scores = []\n",
    "    cooldown_frames = 0  # Allow for brief loss of pose\n",
    "\n",
    "    print(f\"Video dimensions: {frame_width}x{frame_height}, FPS: {fps}\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "        annotated_image = image.copy()\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            good_frames += 1\n",
    "            frames_without_detection = 0\n",
    "            cooldown_frames = 0\n",
    "\n",
    "            # Only draw body landmarks (not face)\n",
    "            body_landmarks = [\n",
    "                mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
    "                mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n",
    "                mp_pose.PoseLandmark.LEFT_ANKLE, mp_pose.PoseLandmark.RIGHT_ANKLE\n",
    "            ]\n",
    "            # Draw only body landmarks and connections\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_image,\n",
    "                results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                             landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n",
    "            left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,\n",
    "                         landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]\n",
    "            left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x,\n",
    "                          landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "            right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]\n",
    "\n",
    "            def calculate_angle(a, b, c):\n",
    "                a = np.array(a)\n",
    "                b = np.array(b)\n",
    "                c = np.array(c)\n",
    "                radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "                angle = np.abs(radians * 180.0 / np.pi)\n",
    "                if angle > 180.0:\n",
    "                    angle = 360 - angle\n",
    "                return angle\n",
    "\n",
    "            # Check body alignment (shoulder-hip-ankle angle should be close to 180)\n",
    "            left_alignment_angle = calculate_angle(left_shoulder, left_hip, left_ankle)\n",
    "            right_alignment_angle = calculate_angle(right_shoulder, right_hip, right_ankle)\n",
    "            alignment_score = (180 - abs(left_alignment_angle - 180) + 180 - abs(right_alignment_angle - 180)) / 360\n",
    "            alignment_scores.append(alignment_score)\n",
    "\n",
    "            cv2.putText(annotated_image, f\"Alignment: {alignment_score * 100:.1f}%\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Determine if plank is active (shoulders above hips, ignore feet)\n",
    "            if left_shoulder[1] < left_hip[1] and right_shoulder[1] < right_hip[1]:\n",
    "                if not plank_active:\n",
    "                    plank_active = True\n",
    "                    start_time = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "                if start_time is not None:\n",
    "                    current_time = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "                    duration = (current_time - start_time) / 1000  # seconds\n",
    "            else:\n",
    "                # Instead of immediately resetting, allow a short cooldown\n",
    "                if plank_active and cooldown_frames < fps // 2:  # allow 0.5s grace\n",
    "                    cooldown_frames += 1\n",
    "                else:\n",
    "                    plank_active = False\n",
    "                    start_time = None\n",
    "                    duration = 0\n",
    "        else:\n",
    "            frames_without_detection += 1\n",
    "            if frames_without_detection % 30 == 0:\n",
    "                print(f\"No pose detection for {frames_without_detection} frames\")\n",
    "            # Allow brief detection loss\n",
    "            if plank_active and cooldown_frames < fps // 2:\n",
    "                cooldown_frames += 1\n",
    "            else:\n",
    "                plank_active = False\n",
    "                start_time = None\n",
    "                duration = 0\n",
    "\n",
    "        cv2.putText(annotated_image, f'Duration: {duration:.1f}s', (10, 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if output_video_path:\n",
    "            out.write(annotated_image)\n",
    "\n",
    "    cap.release()\n",
    "    if output_video_path:\n",
    "        out.release()\n",
    "\n",
    "    if good_frames < 10:\n",
    "        return {\n",
    "            \"duration\": 0,\n",
    "            \"error\": \"Not enough valid pose detections. Check video quality and positioning.\"\n",
    "        }\n",
    "\n",
    "    avg_alignment = sum(alignment_scores) / max(len(alignment_scores), 1)\n",
    "\n",
    "    feedback = {\n",
    "        \"duration\": duration,\n",
    "        \"form_analysis\": {\n",
    "            \"avg_body_alignment_score\": avg_alignment * 100,\n",
    "            \"frames_analyzed\": good_frames\n",
    "        },\n",
    "        \"feedback\": []\n",
    "    }\n",
    "\n",
    "    if avg_alignment < 0.85:\n",
    "        feedback[\"feedback\"].append(\"Maintain a straighter line from your shoulders to your ankles. Avoid sagging or arching your back.\")\n",
    "\n",
    "    if not feedback[\"feedback\"]:\n",
    "        feedback[\"feedback\"].append(\"Great plank form! You held a good straight line.\")\n",
    "\n",
    "    return feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35363d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1745963794.144097 1418972 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745963794.160714 1418972 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1745963794.182820 1418969 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video dimensions: 1280x720, FPS: 30\n",
      "\n",
      "--- Plank Analysis Results ---\n",
      "Total Plank Duration: 61.74125555555556 seconds\n",
      "\n",
      "Form Analysis:\n",
      "  Avg Body Alignment Score: 96.48353582888011\n",
      "  Frames Analyzed: 1854\n",
      "\n",
      "Feedback:\n",
      "- Great plank form! You held a good straight line.\n",
      "\n",
      "Annotated video saved to: planks/dom_output.mov\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with the actual path to your video file\n",
    "    video_file = 'planks/dom_plank.mp4'\n",
    "    # Optional: Specify a path to save the annotated video\n",
    "    output_file = 'planks/dom_output.mov'\n",
    "\n",
    "    try:\n",
    "        analysis_results = analyze_plank(video_file, output_video_path=output_file)\n",
    "\n",
    "        print(\"\\n--- Plank Analysis Results ---\")\n",
    "        if \"error\" in analysis_results:\n",
    "            print(f\"Error: {analysis_results['error']}\")\n",
    "        else:\n",
    "            print(f\"Total Plank Duration: {analysis_results['duration']} seconds\")\n",
    "            print(\"\\nForm Analysis:\")\n",
    "            for key, value in analysis_results['form_analysis'].items():\n",
    "                print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "            print(\"\\nFeedback:\")\n",
    "            for msg in analysis_results['feedback']:\n",
    "                print(f\"- {msg}\")\n",
    "            if output_file:\n",
    "                 print(f\"\\nAnnotated video saved to: {output_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input video file not found at {video_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
